{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjoIvHTuAl_y",
    "outputId": "a0f4cb3f-7211-45a5-8a96-73f7fb3a7aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-plot\n",
      "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-plot) (1.0.2)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-plot) (3.3.4)\n",
      "Requirement already satisfied: scipy>=0.9 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-plot) (1.7.0)\n",
      "Requirement already satisfied: joblib>=0.10 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-plot) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /Users/mac/.local/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.19.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.18->scikit-plot) (2.1.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
      "Installing collected packages: scikit-plot\n",
      "Successfully installed scikit-plot-0.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-plot\n",
    "# !pip uninstall -v scikit-learn\n",
    "# !pip install -v scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhmGGTekVi2c"
   },
   "source": [
    "**Importing required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VHZpZQ9eAztc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFTM_sulVrIA"
   },
   "source": [
    "**Load Data :**\n",
    "\n",
    "Here we will load the dataset, which is collected by the researches manually. we will use this data set to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 704
    },
    "id": "wB1PiW-8A0Dk",
    "outputId": "5d31d966-b34e-4f44-ef93-fd61ea2dc9a0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#AUTHID</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>sEXT</th>\n",
       "      <th>sNEU</th>\n",
       "      <th>sAGR</th>\n",
       "      <th>sCON</th>\n",
       "      <th>sOPN</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "      <th>DATE</th>\n",
       "      <th>NETWORKSIZE</th>\n",
       "      <th>BETWEENNESS</th>\n",
       "      <th>NBETWEENNESS</th>\n",
       "      <th>DENSITY</th>\n",
       "      <th>BROKERAGE</th>\n",
       "      <th>NBROKERAGE</th>\n",
       "      <th>TRANSITIVITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes the sound of thunder.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/19/09 03:21 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is so sleepy it's not even funny that's she ca...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/02/09 08:41 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is sore and wants the knot of muscles at the b...</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/15/09 01:15 PM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>likes how the day sounds in this new song.</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>06/22/09 04:48 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b7b7764cfa1c523e4e93ab2a79a946c4</td>\n",
       "      <td>is home. &lt;3</td>\n",
       "      <td>2.65</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.4</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>07/20/09 02:31 AM</td>\n",
       "      <td>180.0</td>\n",
       "      <td>14861.6</td>\n",
       "      <td>93.29</td>\n",
       "      <td>0.03</td>\n",
       "      <td>15661.0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #AUTHID  \\\n",
       "0  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "1  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "2  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "3  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "4  b7b7764cfa1c523e4e93ab2a79a946c4   \n",
       "\n",
       "                                              STATUS  sEXT  sNEU  sAGR  sCON  \\\n",
       "0                        likes the sound of thunder.  2.65   3.0  3.15  3.25   \n",
       "1  is so sleepy it's not even funny that's she ca...  2.65   3.0  3.15  3.25   \n",
       "2  is sore and wants the knot of muscles at the b...  2.65   3.0  3.15  3.25   \n",
       "3         likes how the day sounds in this new song.  2.65   3.0  3.15  3.25   \n",
       "4                                        is home. <3  2.65   3.0  3.15  3.25   \n",
       "\n",
       "   sOPN cEXT cNEU cAGR cCON cOPN               DATE  NETWORKSIZE  BETWEENNESS  \\\n",
       "0   4.4    n    y    n    n    y  06/19/09 03:21 PM        180.0      14861.6   \n",
       "1   4.4    n    y    n    n    y  07/02/09 08:41 AM        180.0      14861.6   \n",
       "2   4.4    n    y    n    n    y  06/15/09 01:15 PM        180.0      14861.6   \n",
       "3   4.4    n    y    n    n    y  06/22/09 04:48 AM        180.0      14861.6   \n",
       "4   4.4    n    y    n    n    y  07/20/09 02:31 AM        180.0      14861.6   \n",
       "\n",
       "   NBETWEENNESS  DENSITY  BROKERAGE  NBROKERAGE  TRANSITIVITY  \n",
       "0         93.29     0.03    15661.0        0.49           0.1  \n",
       "1         93.29     0.03    15661.0        0.49           0.1  \n",
       "2         93.29     0.03    15661.0        0.49           0.1  \n",
       "3         93.29     0.03    15661.0        0.49           0.1  \n",
       "4         93.29     0.03    15661.0        0.49           0.1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"mypersonality_final.csv\", encoding='cp1252')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Br7WFlHxV0JG"
   },
   "source": [
    "**Analysis of Dataset**\n",
    "\n",
    "here we will do some analysis on loaded dataset, so that we can understand dataset well like what type of data is there, type of the data, missing values, corelation between all the variables etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJxiCTNkV5uQ"
   },
   "source": [
    "**Drop un-necessary columns**\n",
    "\n",
    "here we will remove some un-wanted columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "EGprEmOqBxJo",
    "outputId": "cedc744a-0a84-4130-e5de-5f7e1be235fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9907</th>\n",
       "      <td>Oh well.  Only two things to do when your team...</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9908</th>\n",
       "      <td>And the cabin fever begins.</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9909</th>\n",
       "      <td>Facebook me marea. Me hates it long time T-T</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9910</th>\n",
       "      <td>Red</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9911</th>\n",
       "      <td>About mornings and winter,and magic.</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9912</th>\n",
       "      <td>little things give you away.</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9913</th>\n",
       "      <td>is wishing it was Saturday.</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9914</th>\n",
       "      <td>is studying hard for the G.R.E.</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9915</th>\n",
       "      <td>snipers get more head</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>Last night was amazing! Not only did I see *PR...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 STATUS cEXT cNEU cAGR cCON  \\\n",
       "9907  Oh well.  Only two things to do when your team...    n    n    n    y   \n",
       "9908                        And the cabin fever begins.    n    n    n    y   \n",
       "9909       Facebook me marea. Me hates it long time T-T    n    y    n    n   \n",
       "9910                                                Red    y    n    y    y   \n",
       "9911               About mornings and winter,and magic.    n    n    y    n   \n",
       "9912                       little things give you away.    n    n    y    n   \n",
       "9913                        is wishing it was Saturday.    y    y    y    y   \n",
       "9914                    is studying hard for the G.R.E.    y    y    y    y   \n",
       "9915                              snipers get more head    n    y    n    n   \n",
       "9916  Last night was amazing! Not only did I see *PR...    y    y    n    y   \n",
       "\n",
       "     cOPN  \n",
       "9907    y  \n",
       "9908    y  \n",
       "9909    y  \n",
       "9910    n  \n",
       "9911    y  \n",
       "9912    y  \n",
       "9913    y  \n",
       "9914    y  \n",
       "9915    y  \n",
       "9916    y  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data[[\"STATUS\",\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\"]]\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qIrOyLk4B0hy",
    "outputId": "532426bf-7076-4b59-fd7a-441233303743"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "STATUS    0\n",
       "cEXT      0\n",
       "cNEU      0\n",
       "cAGR      0\n",
       "cCON      0\n",
       "cOPN      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM_7v-zeWAHK"
   },
   "source": [
    "***Data Distribusion chart***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxHuCKd5WLlj"
   },
   "source": [
    "**NLP Pre-Processing**\n",
    "\n",
    "here we will do some nlp pre-processing on data like lower case conversion, drop null values, remove html tags, remove urls , remove numbers, characters, stop words, Contraction to Expansion, stemming etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whF9Ehj6B9R8",
    "outputId": "6ae05db9-1de9-4f86-8065-9d554e893704"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/mac/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Reusable Code for NLP Preprocessing \n",
    "###########\n",
    "# used for dataframe related stufs\n",
    "###########\n",
    "class CleanDataFrame():\n",
    "    \n",
    "    def dropna(self,input_df):\n",
    "        #print(\"DropNA\",input_df.isnull().values.any())\n",
    "        if input_df.isnull().values.any():\n",
    "            return input_df.dropna()\n",
    "        else:\n",
    "            return input_df\n",
    "        \n",
    "    def to_lower(self,input_df):\n",
    "        #print(\"ToLower\")\n",
    "        input_df = input_df.apply(lambda x: x.astype(str).str.lower())\n",
    "        return input_df\n",
    "    \n",
    "    def to_upper(self,input_df):\n",
    "        input_df = input_df.apply(lambda x: x.astype(str).str.upper())\n",
    "        return input_df  \n",
    "\n",
    "    def remove_single_words(self,input_df):\n",
    "        input_df = input_df[[len(i.split())>1 for i in input_df['STATUS'].astype(str).values]]\n",
    "        return input_df\n",
    "    \n",
    "\n",
    "###########\n",
    "# used for text related stufs\n",
    "###########\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "# nltk.download(\"popular\")  #this will be required onetime only\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class TextPreProcessing():\n",
    "    # make all text lowercase\n",
    "    def text_lowercase(self,text):\n",
    "        return text.lower()\n",
    "    # remove HTML tags\n",
    "    def remove_html_tags(self,text):\n",
    "        html_pattern = r'<.*?>'\n",
    "        without_html = re.sub(pattern=html_pattern, repl=' ', string=text)\n",
    "        return without_html\n",
    "    # remove url\n",
    "    def remove_urls(self,text):\n",
    "        new_text = re.sub(r'www\\S+|http\\S+', '', text)\n",
    "        return new_text\n",
    "    # remove numbers\n",
    "    def remove_numbers(self,text):\n",
    "        result = re.sub(r'\\d+', '', text)\n",
    "        return result\n",
    "    \n",
    "    # remove special char\n",
    "    def remove_specialchar(self,text):\n",
    "        result = re.sub('[^A-Za-z0-9]+', '', text)\n",
    "        return result\n",
    "    \n",
    "    # Contraction to Expansion\n",
    "    def contraction_to_expansion(self,text):\n",
    "        contractions = { \n",
    "            \"ain't\": \"am not\",\n",
    "            \"aren't\": \"are not\",\n",
    "            \"can't\": \"cannot\",\n",
    "            \"can't've\": \"cannot have\",\n",
    "            \"'cause\": \"because\",\n",
    "            \"could've\": \"could have\",\n",
    "            \"couldn't\": \"could not\",\n",
    "            \"couldn't've\": \"could not have\",\n",
    "            \"didn't\": \"did not\",\n",
    "            \"doesn't\": \"does not\",\n",
    "            \"don't\": \"do not\",\n",
    "            \"hadn't\": \"had not\",\n",
    "            \"hadn't've\": \"had not have\",\n",
    "            \"hasn't\": \"has not\",\n",
    "            \"haven't\": \"have not\",\n",
    "            \"he'd\": \"he would\",\n",
    "            \"he'd've\": \"he would have\",\n",
    "            \"he'll\": \"he will\",\n",
    "            \"he'll've\": \"he will have\",\n",
    "            \"he's\": \"he is\",\n",
    "            \"how'd\": \"how did\",\n",
    "            \"how'd'y\": \"how do you\",\n",
    "            \"how'll\": \"how will\",\n",
    "            \"how's\": \"how does\",\n",
    "            \"i'd\": \"i would\",\n",
    "            \"i'd've\": \"i would have\",\n",
    "            \"i'll\": \"i will\",\n",
    "            \"i'll've\": \"i will have\",\n",
    "            \"i'm\": \"i am\",\n",
    "            \"i've\": \"i have\",\n",
    "            \"isn't\": \"is not\",\n",
    "            \"it'd\": \"it would\",\n",
    "            \"it'd've\": \"it would have\",\n",
    "            \"it'll\": \"it will\",\n",
    "            \"it'll've\": \"it will have\",\n",
    "            \"it's\": \"it is\",\n",
    "            \"let's\": \"let us\",\n",
    "            \"ma'am\": \"madam\",\n",
    "            \"mayn't\": \"may not\",\n",
    "            \"might've\": \"might have\",\n",
    "            \"mightn't\": \"might not\",\n",
    "            \"mightn't've\": \"might not have\",\n",
    "            \"must've\": \"must have\",\n",
    "            \"mustn't\": \"must not\",\n",
    "            \"mustn't've\": \"must not have\",\n",
    "            \"needn't\": \"need not\",\n",
    "            \"needn't've\": \"need not have\",\n",
    "            \"o'clock\": \"of the clock\",\n",
    "            \"oughtn't\": \"ought not\",\n",
    "            \"oughtn't've\": \"ought not have\",\n",
    "            \"shan't\": \"shall not\",\n",
    "            \"sha'n't\": \"shall not\",\n",
    "            \"shan't've\": \"shall not have\",\n",
    "            \"she'd\": \"she would\",\n",
    "            \"she'd've\": \"she would have\",\n",
    "            \"she'll\": \"she will\",\n",
    "            \"she'll've\": \"she will have\",\n",
    "            \"she's\": \"she is\",\n",
    "            \"should've\": \"should have\",\n",
    "            \"shouldn't\": \"should not\",\n",
    "            \"shouldn't've\": \"should not have\",\n",
    "            \"so've\": \"so have\",\n",
    "            \"so's\": \"so is\",\n",
    "            \"that'd\": \"that would\",\n",
    "            \"that'd've\": \"that would have\",\n",
    "            \"that's\": \"that is\",\n",
    "            \"there'd\": \"there would\",\n",
    "            \"there'd've\": \"there would have\",\n",
    "            \"there's\": \"there is\",\n",
    "            \"they'd\": \"they would\",\n",
    "            \"they'd've\": \"they would have\",\n",
    "            \"they'll\": \"they will\",\n",
    "            \"they'll've\": \"they will have\",\n",
    "            \"they're\": \"they are\",\n",
    "            \"they've\": \"they have\",\n",
    "            \"to've\": \"to have\",\n",
    "            \"wasn't\": \"was not\",\n",
    "            \" u \": \" you \",\n",
    "            \" ur \": \" your \",\n",
    "            \" n \": \" and \",\n",
    "            \"won't\": \"would not\",\n",
    "            'dis': 'this',\n",
    "            'bak': 'back',\n",
    "            'brng': 'bring'}\n",
    "        if type(text) is str:\n",
    "            for key in contractions:\n",
    "                value = contractions[key]\n",
    "                text = text.replace(key,value)\n",
    "            return text\n",
    "        else:\n",
    "            return text\n",
    "    # remove punctuation - The following code removes this set of symbols [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:\n",
    "    def remove_punctuation(self,text):\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    ## Remove single characters\n",
    "    def remove_single_char(self,text):\n",
    "        return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    # tokenize\n",
    "    def tokenize(self,text):\n",
    "        text = word_tokenize(text)\n",
    "        return text\n",
    "    # remove stopwords\n",
    "    def remove_stopwords(self,text): \n",
    "        return \" \".join([t for t in text if t not in STOP_WORDS])\n",
    "    # lemmatize string \n",
    "    def lemmatize_word(self,text): \n",
    "        #word_tokens = word_tokenize(text) \n",
    "        # provide context i.e. part-of-speech\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text] \n",
    "        return lemmas \n",
    "\n",
    "    def preprocessing(self,text):\n",
    "        #text = self.text_lowercase(text)\n",
    "        text = self.remove_urls(text)\n",
    "        text = self.remove_html_tags(text)\n",
    "        text = self.contraction_to_expansion(text)\n",
    "        text = self.remove_numbers(text)\n",
    "        text = self.remove_punctuation(text)\n",
    "        text = self.remove_single_char(text)\n",
    "        text = self.tokenize(text)\n",
    "        text = self.lemmatize_word(text)\n",
    "        text = self.remove_stopwords(text)\n",
    "        return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "61CZPts8CBGT",
    "outputId": "24653326-acc6-47cb-f036-8b670ba8072e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-64c017eae3c8>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['STATUS'] =  df['STATUS'].apply(lambda x: objTextPreProcessing.preprocessing(x)) #apply nlp pre-processing on textcolumn\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>cEXT</th>\n",
       "      <th>cNEU</th>\n",
       "      <th>cAGR</th>\n",
       "      <th>cCON</th>\n",
       "      <th>cOPN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like sound thunder</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sleepy funny sleep</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sore want knot muscle base neck stop hurt on h...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like day sound new song</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>saw nun zombie like also propname tentacleman ...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>kentucky miles mile journey home</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>finish digital paint tablet haywire is contemp...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>celebrate new haircut listen swinger music gen...</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>crush green lantern</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>magic brain</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               STATUS cEXT cNEU cAGR cCON cOPN\n",
       "0                                  like sound thunder    n    y    n    n    y\n",
       "1                                  sleepy funny sleep    n    y    n    n    y\n",
       "2   sore want knot muscle base neck stop hurt on h...    n    y    n    n    y\n",
       "3                             like day sound new song    n    y    n    n    y\n",
       "6   saw nun zombie like also propname tentacleman ...    n    y    n    n    y\n",
       "7                    kentucky miles mile journey home    n    y    n    n    y\n",
       "8   finish digital paint tablet haywire is contemp...    n    y    n    n    y\n",
       "9   celebrate new haircut listen swinger music gen...    n    y    n    n    y\n",
       "10                                crush green lantern    n    y    n    n    y\n",
       "11                                        magic brain    n    y    n    n    y"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean data\n",
    "objTextPreProcessing = TextPreProcessing()\n",
    "df['STATUS'] =  df['STATUS'].apply(lambda x: objTextPreProcessing.preprocessing(x)) #apply nlp pre-processing on textcolumn\n",
    "\n",
    "objCleanDataFrame = CleanDataFrame()\n",
    "df = objCleanDataFrame.dropna(df)   #drop null values from dataframe\n",
    "df = objCleanDataFrame.to_lower(df) #convert dataframe to lowercase\n",
    "df = objCleanDataFrame.remove_single_words(df) #drop single word row from dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1S04IK5DWTQw"
   },
   "source": [
    "**independent and dependent variable split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YaNEoxT7CJui"
   },
   "outputs": [],
   "source": [
    "# independent and dependent variable split\n",
    "X = df[['STATUS']]\n",
    "y = df[[\"cEXT\",\"cNEU\",\"cAGR\",\"cCON\",\"cOPN\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L63uoZElWZ03"
   },
   "source": [
    "**NLP Columntransformer**\n",
    "\n",
    "here we are converting text column into Numbers so that machine can understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "u2EyWF-xUN_3"
   },
   "outputs": [],
   "source": [
    "# Word2Vec Model \n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "class GensimWord2VecVectorizer(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    def __init__(self, size=100, alpha=0.025, window=5, min_count=5, max_vocab_size=None,\n",
    "                sample=0.001, seed=1, workers=3, min_alpha=0.0001, sg=0, hs=0, negative=5,\n",
    "                ns_exponent=0.75, cbow_mean=1, hashfxn=hash, iter=5, null_word=0,\n",
    "                trim_rule=None, sorted_vocab=1, batch_words=10000, compute_loss=False,\n",
    "                callbacks=(), max_final_vocab=None):\n",
    "        self.size = size\n",
    "        self.alpha = alpha\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.sample = sample\n",
    "        self.seed = seed\n",
    "        self.workers = workers\n",
    "        self.min_alpha = min_alpha\n",
    "        self.sg = sg\n",
    "        self.hs = hs\n",
    "        self.negative = negative\n",
    "        self.ns_exponent = ns_exponent\n",
    "        self.cbow_mean = cbow_mean\n",
    "        self.hashfxn = hashfxn\n",
    "        self.iter = iter\n",
    "        self.null_word = null_word\n",
    "        self.trim_rule = trim_rule\n",
    "        self.sorted_vocab = sorted_vocab\n",
    "        self.batch_words = batch_words\n",
    "        self.compute_loss = compute_loss\n",
    "        self.callbacks = callbacks\n",
    "        self.max_final_vocab = max_final_vocab\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model_ = Word2Vec(\n",
    "            sentences=X, corpus_file=None,\n",
    "            size=self.size, alpha=self.alpha, window=self.window, min_count=self.min_count,\n",
    "            max_vocab_size=self.max_vocab_size, sample=self.sample, seed=self.seed,\n",
    "            workers=self.workers, min_alpha=self.min_alpha, sg=self.sg, hs=self.hs,\n",
    "            negative=self.negative, ns_exponent=self.ns_exponent, cbow_mean=self.cbow_mean,\n",
    "            hashfxn=self.hashfxn, iter=self.iter, null_word=self.null_word,\n",
    "            trim_rule=self.trim_rule, sorted_vocab=self.sorted_vocab, batch_words=self.batch_words,\n",
    "            compute_loss=self.compute_loss, callbacks=self.callbacks,\n",
    "            max_final_vocab=self.max_final_vocab)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_embeddings = np.array([self._get_embedding(words) for words in X])\n",
    "        return X_embeddings\n",
    "\n",
    "    def _get_embedding(self, words):\n",
    "        valid_words = [word for word in words if word in self.model_.wv.vocab]\n",
    "        if valid_words:\n",
    "            embedding = np.zeros((len(valid_words), self.size), dtype=np.float32)\n",
    "            for idx, word in enumerate(valid_words):\n",
    "                embedding[idx] = self.model_.wv[word]\n",
    "\n",
    "            return np.mean(embedding, axis=0)\n",
    "        else:\n",
    "            return np.zeros(self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ynp7_t_38Rbr"
   },
   "outputs": [],
   "source": [
    "from WebApp.app import GensimWord2VecVectorizer\n",
    "GensimWord2VecVectorizer.__module__ = 'app'\n",
    "\n",
    "import sys\n",
    "sys.modules['app'] = sys.modules['WebApp.app']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tSqWJf8rxfvU"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "        ('step1', GensimWord2VecVectorizer(), 'STATUS')\n",
    "    ],remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYc569_LWqJ5"
   },
   "source": [
    "**check data for imbalance**\n",
    "\n",
    "it is very important to check data balance otherwise it will give you unexpected result or bias result in favour of majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9XFPhFfQVw9Z",
    "outputId": "9c34075b-1d14-4f9b-ca25-dac88e680061"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n' 'y'] [5338 3983]\n",
      "['n' 'y'] [5842 3479]\n",
      "['n' 'y'] [4341 4980]\n",
      "['n' 'y'] [5041 4280]\n",
      "['n' 'y'] [2378 6943]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y[\"cEXT\"], return_counts=True)\n",
    "print(unique,counts)\n",
    "unique, counts = np.unique(y[\"cNEU\"], return_counts=True)\n",
    "print(unique,counts)\n",
    "unique, counts = np.unique(y[\"cAGR\"], return_counts=True)\n",
    "print(unique,counts)\n",
    "unique, counts = np.unique(y[\"cCON\"], return_counts=True)\n",
    "print(unique,counts)\n",
    "unique, counts = np.unique(y[\"cOPN\"], return_counts=True)\n",
    "print(unique,counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wXcYPJkrW_ZM",
    "outputId": "ad2076e6-5827-43a1-dfc6-6899d71bcaf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3983, 6)\n",
      "(5338, 6)\n",
      "(3983, 6)\n",
      "(7966, 6)\n",
      "['n' 'y'] [3983 3983]\n"
     ]
    }
   ],
   "source": [
    "# Balancing the cEXT Class\n",
    "df_cEXT_y = df[df['cEXT']=='y']\n",
    "df_cEXT_n = df[df['cEXT']=='n']\n",
    "print(df_cEXT_y.shape)\n",
    "print(df_cEXT_n.shape)\n",
    "\n",
    "df_cEXT_downsampled = df_cEXT_n.sample(df_cEXT_y.shape[0])\n",
    "print(df_cEXT_downsampled.shape)\n",
    "\n",
    "df_cEXT_balanced = pd.concat([df_cEXT_downsampled, df_cEXT_y])\n",
    "print(df_cEXT_balanced.shape)\n",
    "unique, counts = np.unique(df_cEXT_balanced[\"cEXT\"], return_counts=True)\n",
    "print(unique,counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-GW1LNPxk7Y"
   },
   "source": [
    "#cEXT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9QF6IQHIh5BJ"
   },
   "outputs": [],
   "source": [
    "# independent and dependent variable split for cEXT\n",
    "X_cEXT = df_cEXT_balanced[['STATUS']]\n",
    "y_cEXT = df_cEXT_balanced[[\"cEXT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "IlAE9oxjCMLi",
    "outputId": "125550b0-cffc-41cf-f630-fb02cc479a9a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>wonder better idea stay home celebrate hemingw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9397</th>\n",
       "      <td>aaaaaagggggggggghhhhhhhh hurt bad past days wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>fb wierd ffs way home page look funny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>massive robotinduced coronary day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>those owe resume hand them in gon na night ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 STATUS\n",
       "1143  wonder better idea stay home celebrate hemingw...\n",
       "9397  aaaaaagggggggggghhhhhhhh hurt bad past days wi...\n",
       "2534              fb wierd ffs way home page look funny\n",
       "2748                  massive robotinduced coronary day\n",
       "2575  those owe resume hand them in gon na night ton..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_cEXT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Vu3Ni-DmCO8k",
    "outputId": "28bc7fc8-3188-42f6-e9f7-16b8276c7872"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1143</th>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9397</th>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2534</th>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2748</th>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cEXT\n",
       "1143    n\n",
       "9397    n\n",
       "2534    n\n",
       "2748    n\n",
       "2575    n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_cEXT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "D5VMHjFCCQ7t"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cEXT,y_cEXT,test_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mGfAVPphyUI3"
   },
   "outputs": [],
   "source": [
    "X_test = pd.concat([X_train,X_test])\n",
    "y_test = pd.concat([y_train,y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "qU4fG4caCXhG"
   },
   "outputs": [],
   "source": [
    "#convert target variable into number\n",
    "y_train.replace({'y':1, 'n':0}, inplace = True)\n",
    "y_test.replace({'y':1, 'n':0}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5zeUcaIlqje",
    "outputId": "5156b1a0-ce65-4992-fd5f-4bf8d03f8c0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV 1/5] END C=0.1, gamma=1, kernel=rbf, probability=True;, score=0.504 total time=   2.6s\n",
      "[CV 2/5] END C=0.1, gamma=1, kernel=rbf, probability=True;, score=0.504 total time=   2.7s\n",
      "[CV 3/5] END C=0.1, gamma=1, kernel=rbf, probability=True;, score=0.504 total time=   2.5s\n",
      "[CV 4/5] END C=0.1, gamma=1, kernel=rbf, probability=True;, score=0.504 total time=   2.7s\n",
      "[CV 5/5] END C=0.1, gamma=1, kernel=rbf, probability=True;, score=0.505 total time=   2.5s\n",
      "[CV 1/5] END C=0.1, gamma=0.1, kernel=rbf, probability=True;, score=0.504 total time=   2.8s\n",
      "[CV 2/5] END C=0.1, gamma=0.1, kernel=rbf, probability=True;, score=0.504 total time=   2.9s\n",
      "[CV 3/5] END C=0.1, gamma=0.1, kernel=rbf, probability=True;, score=0.504 total time=   3.3s\n",
      "[CV 4/5] END C=0.1, gamma=0.1, kernel=rbf, probability=True;, score=0.504 total time=   3.1s\n",
      "[CV 5/5] END C=0.1, gamma=0.1, kernel=rbf, probability=True;, score=0.505 total time=   3.6s\n",
      "[CV 1/5] END C=1, gamma=1, kernel=rbf, probability=True;, score=0.504 total time=   3.1s\n",
      "[CV 2/5] END C=1, gamma=1, kernel=rbf, probability=True;, score=0.504 total time=   3.1s\n",
      "[CV 3/5] END C=1, gamma=1, kernel=rbf, probability=True;, score=0.504 total time=   3.3s\n",
      "[CV 4/5] END C=1, gamma=1, kernel=rbf, probability=True;, score=0.504 total time=   3.5s\n",
      "[CV 5/5] END C=1, gamma=1, kernel=rbf, probability=True;, score=0.505 total time=   3.7s\n",
      "[CV 1/5] END C=1, gamma=0.1, kernel=rbf, probability=True;, score=0.504 total time=   4.2s\n",
      "[CV 2/5] END C=1, gamma=0.1, kernel=rbf, probability=True;, score=0.504 total time=   3.7s\n",
      "[CV 3/5] END C=1, gamma=0.1, kernel=rbf, probability=True;, score=0.504 total time=   4.3s\n",
      "[CV 4/5] END C=1, gamma=0.1, kernel=rbf, probability=True;, score=0.504 total time=   3.8s\n",
      "[CV 5/5] END C=1, gamma=0.1, kernel=rbf, probability=True;, score=0.505 total time=   2.8s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,recall_score, classification_report, cohen_kappa_score\n",
    "from sklearn import metrics \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "import numpy as np\n",
    "import dill\n",
    "import torch\n",
    "# ########## \n",
    "# SVC - support vector classifier \n",
    "# ##########\n",
    "# defining parameter range\n",
    "hyperparameters = {'C': [0.1, 1],\n",
    "              'gamma': [1, 0.1],\n",
    "              'kernel': ['rbf'],\n",
    "              'probability':[True]}\n",
    "model_sv = Pipeline([\n",
    "        ('column_transformers', ct),\n",
    "        ('model', GridSearchCV(SVC(), hyperparameters,\n",
    "                               refit = True, verbose = 3)),\n",
    "    ])\n",
    "model_sv_cEXT = model_sv.fit(X_train, y_train['cEXT'])\n",
    "# Save the trained cEXT - SVM Model.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KIEwBcubDPkG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_Word2Vec_sv_cEXT.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model_sv_cEXT, 'model_Word2Vec_sv_cEXT.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8ZdUP4YbFMI",
    "outputId": "340bbc14-3d05-44e1-d1aa-592aa40aef53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sv_cEXT.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6i7AhlBpdng",
    "outputId": "2a2b6c92-a57d-4ee5-d0b0-61fc9c219175"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52825251, 0.47174749],\n",
       "       [0.52823763, 0.47176237],\n",
       "       [0.5282414 , 0.4717586 ],\n",
       "       ...,\n",
       "       [0.52823388, 0.47176612],\n",
       "       [0.5282419 , 0.4717581 ],\n",
       "       [0.52826144, 0.47173856]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sv_cEXT.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tO174jMCJ_w8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML_Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
